{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Developer Tools \u00b6","title":"Welcome to Developer Tools"},{"location":"#welcome-to-developer-tools","text":"","title":"Welcome to Developer Tools"},{"location":"pages/docker/swarm/","text":"Docker Swarm \u00b6 Obtenir l'ID des containers du cluster \u00b6 #!/bin/bash service_names = $( docker service ls --format \"{{.Name}}\" ) for service_name in $service_names ; do echo -e \"\\nservice name : $service_name \\n\" task_ids = $( docker service ps --filter \"desired-state=running\" $service_name --format \"{{.ID}}\" ) for task_id in $task_ids ; do container_id = $( docker inspect --format \"{{.Status.ContainerStatus.ContainerID}}\" $task_id | cut -c1-12 ) echo \"container id : $container_id \" done done","title":"Swarm"},{"location":"pages/docker/swarm/#docker-swarm","text":"","title":"Docker Swarm"},{"location":"pages/docker/swarm/#obtenir-lid-des-containers-du-cluster","text":"#!/bin/bash service_names = $( docker service ls --format \"{{.Name}}\" ) for service_name in $service_names ; do echo -e \"\\nservice name : $service_name \\n\" task_ids = $( docker service ps --filter \"desired-state=running\" $service_name --format \"{{.ID}}\" ) for task_id in $task_ids ; do container_id = $( docker inspect --format \"{{.Status.ContainerStatus.ContainerID}}\" $task_id | cut -c1-12 ) echo \"container id : $container_id \" done done","title":"Obtenir l'ID des containers du cluster"},{"location":"pages/docker/utils/","text":"Utiles \u00b6 Commandes utiles \u00b6 Ajouter un utilisateur au groupe docker usermod -aG docker <username> Supprimer tous les container docker rm -f $( docker ps -aq ) Supprimer toute les images Warning Vous devez supprimer tous les conteneurs avant de supprimer toutes les images \u00e0 partir desquelles ces conteneurs ont \u00e9t\u00e9 cr\u00e9\u00e9s. docker rmi -f $( docker images -aq )","title":"Utiles"},{"location":"pages/docker/utils/#utiles","text":"","title":"Utiles"},{"location":"pages/docker/utils/#commandes-utiles","text":"Ajouter un utilisateur au groupe docker usermod -aG docker <username> Supprimer tous les container docker rm -f $( docker ps -aq ) Supprimer toute les images Warning Vous devez supprimer tous les conteneurs avant de supprimer toutes les images \u00e0 partir desquelles ces conteneurs ont \u00e9t\u00e9 cr\u00e9\u00e9s. docker rmi -f $( docker images -aq )","title":"Commandes utiles"},{"location":"pages/hadoop/install-wsl/","text":"Installation sur WSL \u00b6 Structure des fichiers \u00b6 . \u2514\u2500 hadoop-install/ \u251c\u2500 start.sh \u2514\u2500 config.sh Etapes de lancement \u00b6 Lancer le script Warning Le script doit etre lanc\u00e9 en tant que root ou sudoers cd hadoop-install && sudo ./start.sh Se connecter \u00e0 l'utilisateur hadoop su - hadoop Formater le namenode ${ HADOOP_HOME } /bin/hdfs namenode -format D\u00e9marrer hadoop hadoop-start.sh Code \u00b6 start.sh config.sh #!/bin/bash hadoop_user = \"hadoop\" hadoop_user_home = \"/home/ ${ hadoop_user } \" hadoop_password = \"ipm\" hadoop_version = \"3.3.0\" hadoop_home = \"/usr/local/hadoop- ${ hadoop_version } \" hadoop_url = \"https://archive.apache.org/dist/hadoop/common/hadoop- ${ hadoop_version } /hadoop- ${ hadoop_version } .tar.gz\" sqoop_version = \"1.4.7\" sqoop_hadoop_version = \"2.6.0\" sqoop_home = \"/usr/local/sqoop- ${ sqoop_version } .bin__hadoop- ${ sqoop_hadoop_version } \" sqoop_url = \"https://archive.apache.org/dist/sqoop/ ${ sqoop_version } /sqoop- ${ sqoop_version } .bin__hadoop- ${ sqoop_hadoop_version } .tar.gz\" mysql_driver_url = \"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz\" bashrc = \" ${ hadoop_user_home } /.bashrc\" if [ \" $( id -u ) \" -eq 0 ] ; then #################### # INSTALL PACKAGES # #################### apt remove -y openssh-server && \\ apt-get update && apt-get install -y --no-install-recommends \\ openjdk-8-jdk curl net-tools gnupg openssh-server \\ && rm -rf /var/lib/apt/lists/* \\ && service ssh start || exit 1 ################### # ADD HADOOP USER # ################### useradd \\ --create-home \\ --password $( openssl passwd -1 ${ hadoop_password } ) \\ --shell /bin/bash \\ --groups sudo \\ ${ hadoop_user } || exit 1 ################# # CONFIGURE SSH # ################# mkdir ${ hadoop_user_home } /.ssh && \\ ssh-keygen -t rsa -P '' -f ${ hadoop_user_home } /.ssh/id_rsa && \\ cat ${ hadoop_user_home } /.ssh/id_rsa.pub >> ${ hadoop_user_home } /.ssh/authorized_keys && \\ chmod 0700 ${ hadoop_user_home } /.ssh && \\ chmod 0600 ${ hadoop_user_home } /.ssh/id_rsa && \\ chmod 0644 ${ hadoop_user_home } /.ssh/id_rsa.pub && \\ chmod 0644 ${ hadoop_user_home } /.ssh/authorized_keys && \\ chown -R ${ hadoop_user } : ${ hadoop_user } ${ hadoop_user_home } /.ssh || exit 1 ######################### # INSTALL HADOOP BINARY # ######################### curl -O https://dist.apache.org/repos/dist/release/hadoop/common/KEYS && \\ gpg --import KEYS && rm KEYS && \\ curl -fSL \" ${ hadoop_url } \" -o /tmp/hadoop.tar.gz && \\ curl -fSL \" ${ hadoop_url } .asc\" -o /tmp/hadoop.tar.gz.asc && \\ gpg --verify /tmp/hadoop.tar.gz.asc && \\ tar -xf /tmp/hadoop.tar.gz -C /usr/local/ && \\ rm /tmp/hadoop.tar.gz* && \\ mkdir -p ${ hadoop_home } /logs || exit 1 ######################## # INSTALL SQOOP BINARY # ######################## curl -O https://archive.apache.org/dist/sqoop/KEYS && \\ gpg --import KEYS && rm KEYS && \\ curl -fSL \" ${ sqoop_url } \" -o /tmp/sqoop.tar.gz && \\ curl -fSL \" ${ sqoop_url } .asc\" -o /tmp/sqoop.tar.gz.asc && \\ gpg --verify /tmp/sqoop.tar.gz.asc && \\ tar -xf /tmp/sqoop.tar.gz -C /usr/local/ && \\ rm /tmp/sqoop.tar.gz* && \\ wget ${ mysql_driver_url } && \\ tar -xzf mysql-connector-java-5.1.48.tar.gz cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48.jar ${ sqoop_home } /lib && \\ rm -rf mysql-connector-java-5.1.48* || exit 1 ############################ # SET ENVIRONMENT VARIABLE # ############################ echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> ${ bashrc } && \\ echo \"export HADOOP_HOME= ${ hadoop_home } \" >> ${ bashrc } && \\ echo \"export SQOOP_HOME= ${ sqoop_home } \" >> ${ bashrc } && \\ echo 'export PATH=$PATH:$HADOOP_HOME/bin:' >> ${ bashrc } && \\ echo 'export PATH=$PATH:$HADOOP_HOME/sbin' >> ${ bashrc } && \\ echo 'export PATH=$PATH:$SQOOP_HOME/bin' >> ${ bashrc } && \\ echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> ${ bashrc } && \\ echo 'export HADOOP_MAPRED_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ echo 'export HADOOP_COMMON_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ echo 'export HADOOP_HDFS_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ echo 'export YARN_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ source ${ bashrc } || exit 1 #################### # CONFIGURE HADOOP # #################### sed -i 's/^[ #]*export\\s*JAVA_HOME*\\s*=\\s*.*/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64/' \\ /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh || exit 1 ./config.sh ${ hadoop_home } | exit 1 ########################### # HADOOP USER PERMISSIONS # ########################### chown -R ${ hadoop_user } : ${ hadoop_user } ${ hadoop_home } # /usr/local/hadoop-3.3.0/bin/hdfs namenode -format else echo 'The script must be run as root.' > & 2 fi #!/bin/bash hadoop_home = $1 ################# # core-site.xml # ################# cat << 'EOF' > ${hadoop_home}/etc/hadoop/core-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>fs.defaultFS</name> <value>hdfs://localhost:9000</value> <description>Serveur de donn\u00e9es</description> </property> </configuration> EOF ################# # hdfs-site.xml # ################# cat << 'EOF' > ${hadoop_home}/etc/hadoop/hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>dfs.replication</name> <value>1</value> </property> </configuration> EOF ################### # mapred-site.xml # ################### cat << 'EOF' > ${hadoop_home}/etc/hadoop/mapred-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <property> <name>mapreduce.application.classpath</name> <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value> </property> </configuration> EOF ################# # yarn-site.xml # ################# cat << 'EOF' > ${hadoop_home}/etc/hadoop/yarn-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.env-whitelist</name> <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value> </property> </configuration> EOF ################## # hadoop-start.sh # ################## cat << 'EOF' > ${hadoop_home}/sbin/hadoop-start.sh && chmod +x ${hadoop_home}/sbin/hadoop-start.sh #!/bin/bash ${HADOOP_HOME}/sbin/start-dfs.sh ${HADOOP_HOME}/sbin/start-yarn.sh EOF ################## # hadoop-stop.sh # ################## cat << 'EOF' > ${hadoop_home}/sbin/hadoop-stop.sh && chmod +x ${hadoop_home}/sbin/hadoop-stop.sh #!/bin/bash ${HADOOP_HOME}/sbin/stop-yarn.sh ${HADOOP_HOME}/sbin/stop-dfs.sh EOF","title":"Installation sur WSL"},{"location":"pages/hadoop/install-wsl/#installation-sur-wsl","text":"","title":"Installation sur WSL"},{"location":"pages/hadoop/install-wsl/#structure-des-fichiers","text":". \u2514\u2500 hadoop-install/ \u251c\u2500 start.sh \u2514\u2500 config.sh","title":"Structure des fichiers"},{"location":"pages/hadoop/install-wsl/#etapes-de-lancement","text":"Lancer le script Warning Le script doit etre lanc\u00e9 en tant que root ou sudoers cd hadoop-install && sudo ./start.sh Se connecter \u00e0 l'utilisateur hadoop su - hadoop Formater le namenode ${ HADOOP_HOME } /bin/hdfs namenode -format D\u00e9marrer hadoop hadoop-start.sh","title":"Etapes de lancement"},{"location":"pages/hadoop/install-wsl/#code","text":"start.sh config.sh #!/bin/bash hadoop_user = \"hadoop\" hadoop_user_home = \"/home/ ${ hadoop_user } \" hadoop_password = \"ipm\" hadoop_version = \"3.3.0\" hadoop_home = \"/usr/local/hadoop- ${ hadoop_version } \" hadoop_url = \"https://archive.apache.org/dist/hadoop/common/hadoop- ${ hadoop_version } /hadoop- ${ hadoop_version } .tar.gz\" sqoop_version = \"1.4.7\" sqoop_hadoop_version = \"2.6.0\" sqoop_home = \"/usr/local/sqoop- ${ sqoop_version } .bin__hadoop- ${ sqoop_hadoop_version } \" sqoop_url = \"https://archive.apache.org/dist/sqoop/ ${ sqoop_version } /sqoop- ${ sqoop_version } .bin__hadoop- ${ sqoop_hadoop_version } .tar.gz\" mysql_driver_url = \"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz\" bashrc = \" ${ hadoop_user_home } /.bashrc\" if [ \" $( id -u ) \" -eq 0 ] ; then #################### # INSTALL PACKAGES # #################### apt remove -y openssh-server && \\ apt-get update && apt-get install -y --no-install-recommends \\ openjdk-8-jdk curl net-tools gnupg openssh-server \\ && rm -rf /var/lib/apt/lists/* \\ && service ssh start || exit 1 ################### # ADD HADOOP USER # ################### useradd \\ --create-home \\ --password $( openssl passwd -1 ${ hadoop_password } ) \\ --shell /bin/bash \\ --groups sudo \\ ${ hadoop_user } || exit 1 ################# # CONFIGURE SSH # ################# mkdir ${ hadoop_user_home } /.ssh && \\ ssh-keygen -t rsa -P '' -f ${ hadoop_user_home } /.ssh/id_rsa && \\ cat ${ hadoop_user_home } /.ssh/id_rsa.pub >> ${ hadoop_user_home } /.ssh/authorized_keys && \\ chmod 0700 ${ hadoop_user_home } /.ssh && \\ chmod 0600 ${ hadoop_user_home } /.ssh/id_rsa && \\ chmod 0644 ${ hadoop_user_home } /.ssh/id_rsa.pub && \\ chmod 0644 ${ hadoop_user_home } /.ssh/authorized_keys && \\ chown -R ${ hadoop_user } : ${ hadoop_user } ${ hadoop_user_home } /.ssh || exit 1 ######################### # INSTALL HADOOP BINARY # ######################### curl -O https://dist.apache.org/repos/dist/release/hadoop/common/KEYS && \\ gpg --import KEYS && rm KEYS && \\ curl -fSL \" ${ hadoop_url } \" -o /tmp/hadoop.tar.gz && \\ curl -fSL \" ${ hadoop_url } .asc\" -o /tmp/hadoop.tar.gz.asc && \\ gpg --verify /tmp/hadoop.tar.gz.asc && \\ tar -xf /tmp/hadoop.tar.gz -C /usr/local/ && \\ rm /tmp/hadoop.tar.gz* && \\ mkdir -p ${ hadoop_home } /logs || exit 1 ######################## # INSTALL SQOOP BINARY # ######################## curl -O https://archive.apache.org/dist/sqoop/KEYS && \\ gpg --import KEYS && rm KEYS && \\ curl -fSL \" ${ sqoop_url } \" -o /tmp/sqoop.tar.gz && \\ curl -fSL \" ${ sqoop_url } .asc\" -o /tmp/sqoop.tar.gz.asc && \\ gpg --verify /tmp/sqoop.tar.gz.asc && \\ tar -xf /tmp/sqoop.tar.gz -C /usr/local/ && \\ rm /tmp/sqoop.tar.gz* && \\ wget ${ mysql_driver_url } && \\ tar -xzf mysql-connector-java-5.1.48.tar.gz cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48.jar ${ sqoop_home } /lib && \\ rm -rf mysql-connector-java-5.1.48* || exit 1 ############################ # SET ENVIRONMENT VARIABLE # ############################ echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> ${ bashrc } && \\ echo \"export HADOOP_HOME= ${ hadoop_home } \" >> ${ bashrc } && \\ echo \"export SQOOP_HOME= ${ sqoop_home } \" >> ${ bashrc } && \\ echo 'export PATH=$PATH:$HADOOP_HOME/bin:' >> ${ bashrc } && \\ echo 'export PATH=$PATH:$HADOOP_HOME/sbin' >> ${ bashrc } && \\ echo 'export PATH=$PATH:$SQOOP_HOME/bin' >> ${ bashrc } && \\ echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> ${ bashrc } && \\ echo 'export HADOOP_MAPRED_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ echo 'export HADOOP_COMMON_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ echo 'export HADOOP_HDFS_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ echo 'export YARN_HOME=$HADOOP_HOME' >> ${ bashrc } && \\ source ${ bashrc } || exit 1 #################### # CONFIGURE HADOOP # #################### sed -i 's/^[ #]*export\\s*JAVA_HOME*\\s*=\\s*.*/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64/' \\ /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh || exit 1 ./config.sh ${ hadoop_home } | exit 1 ########################### # HADOOP USER PERMISSIONS # ########################### chown -R ${ hadoop_user } : ${ hadoop_user } ${ hadoop_home } # /usr/local/hadoop-3.3.0/bin/hdfs namenode -format else echo 'The script must be run as root.' > & 2 fi #!/bin/bash hadoop_home = $1 ################# # core-site.xml # ################# cat << 'EOF' > ${hadoop_home}/etc/hadoop/core-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>fs.defaultFS</name> <value>hdfs://localhost:9000</value> <description>Serveur de donn\u00e9es</description> </property> </configuration> EOF ################# # hdfs-site.xml # ################# cat << 'EOF' > ${hadoop_home}/etc/hadoop/hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>dfs.replication</name> <value>1</value> </property> </configuration> EOF ################### # mapred-site.xml # ################### cat << 'EOF' > ${hadoop_home}/etc/hadoop/mapred-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <property> <name>mapreduce.application.classpath</name> <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value> </property> </configuration> EOF ################# # yarn-site.xml # ################# cat << 'EOF' > ${hadoop_home}/etc/hadoop/yarn-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.env-whitelist</name> <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value> </property> </configuration> EOF ################## # hadoop-start.sh # ################## cat << 'EOF' > ${hadoop_home}/sbin/hadoop-start.sh && chmod +x ${hadoop_home}/sbin/hadoop-start.sh #!/bin/bash ${HADOOP_HOME}/sbin/start-dfs.sh ${HADOOP_HOME}/sbin/start-yarn.sh EOF ################## # hadoop-stop.sh # ################## cat << 'EOF' > ${hadoop_home}/sbin/hadoop-stop.sh && chmod +x ${hadoop_home}/sbin/hadoop-stop.sh #!/bin/bash ${HADOOP_HOME}/sbin/stop-yarn.sh ${HADOOP_HOME}/sbin/stop-dfs.sh EOF","title":"Code"},{"location":"pages/java/gson/","text":"S\u00e9rialisation \u00b6 Gson \u00b6 D\u00e9pendence \u00b6 <dependency> <groupId> com.google.code.gson </groupId> <artifactId> gson </artifactId> <version> 2.8.9 </version> </dependency> Impl\u00e9mentation \u00b6 Mod\u00e8le \u00b6 public class Person { private String fistName ; private String lastName ; private int age ; public Person () {} public Person ( String firstName , String lastName , int age ) { this . firstName = firstName ; this . lastName = lastName ; this . age = age ; } } S\u00e9rialisation (Objet => Json) \u00b6 Gson gson = new GsonBuilder () . setPrettyPrinting () . create () Person person = new Person ( \"Ruddy\" , \"Monlouis\" , 24 ) // get json string String json = gson . toJson ( person , person . getClass ()); // save to .json file File jsonFile = new File ( \"/path/to/file.json\" ); try ( PrintWriter pWriter = new PrintWriter ( jsonFile )) { JsonWriter jWriter = gson . newJsonWriter ( pWriter ); jWriter . setIndent ( \"\\t\" ); gson . toJson ( threadInfos , threadInfos . getClass (), jWriter ); pWriter . println (); } S\u00e9rialisation (Json => Objet) \u00b6 Person person = new Person (); Gson gson = new Gson (); FileReader fileReader = new FileReader ( \"/path/to/file.json\" ); try ( JsonReader reader = new JsonReader ( fileReader )) { person = gson . fromJson ( reader , person . getClass ()); }","title":"Gson"},{"location":"pages/java/gson/#serialisation","text":"","title":"S\u00e9rialisation"},{"location":"pages/java/gson/#gson","text":"","title":"Gson"},{"location":"pages/java/gson/#dependence","text":"<dependency> <groupId> com.google.code.gson </groupId> <artifactId> gson </artifactId> <version> 2.8.9 </version> </dependency>","title":"D\u00e9pendence"},{"location":"pages/java/gson/#implementation","text":"","title":"Impl\u00e9mentation"},{"location":"pages/java/gson/#modele","text":"public class Person { private String fistName ; private String lastName ; private int age ; public Person () {} public Person ( String firstName , String lastName , int age ) { this . firstName = firstName ; this . lastName = lastName ; this . age = age ; } }","title":"Mod\u00e8le"},{"location":"pages/java/gson/#serialisation-objet-json","text":"Gson gson = new GsonBuilder () . setPrettyPrinting () . create () Person person = new Person ( \"Ruddy\" , \"Monlouis\" , 24 ) // get json string String json = gson . toJson ( person , person . getClass ()); // save to .json file File jsonFile = new File ( \"/path/to/file.json\" ); try ( PrintWriter pWriter = new PrintWriter ( jsonFile )) { JsonWriter jWriter = gson . newJsonWriter ( pWriter ); jWriter . setIndent ( \"\\t\" ); gson . toJson ( threadInfos , threadInfos . getClass (), jWriter ); pWriter . println (); }","title":"S\u00e9rialisation (Objet =&gt; Json)"},{"location":"pages/java/gson/#serialisation-json-objet","text":"Person person = new Person (); Gson gson = new Gson (); FileReader fileReader = new FileReader ( \"/path/to/file.json\" ); try ( JsonReader reader = new JsonReader ( fileReader )) { person = gson . fromJson ( reader , person . getClass ()); }","title":"S\u00e9rialisation (Json =&gt; Objet)"},{"location":"pages/java/java_agent/","text":"Agent java \u00b6 La plupart des outils de monitoring fonctionne \u00e0 base d\u2019agent. Un agent Java est tout simplement une application Java sous forme d\u2019un fichier .jar. Utiliser un agent permet d\u2019instrumenter un programme d\u2019ex\u00e9cutant dans une JVM en interceptant le chargement des classes et en modifiant le byte code si n\u00e9cessaire. Fichier manifeste \u00b6 Une application packag\u00e9e en .jar est en g\u00e9n\u00e9ral constitu\u00e9 d\u2019un fichier MANIFEST.MF \u00e0 l\u2019int\u00e9rieur d\u2019un dossier META-INF situ\u00e9 \u00e0 la racine. Ce fichier permet de renseigner \u00e0 la JVM, les informations d\u2019ex\u00e9cution sur le fichier (classe principale, classpath, etc\u2026). Si ce fichier n\u2019est pas pr\u00e9sent, toutes les informations li\u00e9es \u00e0 l\u2019ex\u00e9cution de l\u2019application devront \u00eatre renseign\u00e9es manuellement au lancement de celle-ci. Chaque ligne correspond \u00e0 un param\u00e8tre associ\u00e9 \u00e0 sa valeur, sous la forme suivante\u202f: \u00ab\u202fparam\u00e8tre: valeur\u202f\u00bb. Le manifeste d\u2019un agent est un peu similaire \u00e0 celui d\u2019une application classique, mais avec quelque information en plus. Manifest - Version : 1.0 Premain - Class : com . orange . spirit . agent . Agent Agent - Class : com . orange . spirit . agent . Agent Main - Class : com . orange . spirit . app . Main L\u2019exemple ci-dessus nous montre le contenu du manifeste d\u2019un agent Java. Les param\u00e8tres propres \u00e0 un agent sont les suivants : Premain-Class permet d\u2019indiquer le chemin vers la classe portant la m\u00e9thode premain . Agent-Class permet d\u2019indiquer le chemin vers la classe portant la m\u00e9thode agentmain . M\u00e9thodes premain & agentmain \u00b6 La m\u00e9thode premain est appel\u00e9 lorsque l\u2019agent est lanc\u00e9 de mani\u00e8re statique . Et la m\u00e9thode agentmain est appel\u00e9 lorsque l\u2019agent est lanc\u00e9 de mani\u00e8re dynamique . Les deux m\u00e9thodes constituent le point d\u2019entrer d\u2019un agent Java et ont tous deux les m\u00eames signatures. public class Agent { public static void premain ( String args , Instrumentation inst ) { // code ... } public static void agentmain ( String args , Instrumentation inst ) { // code ... } } String args : Ce sont les options pass\u00e9es en arguments dans la ligne de commande lors de l\u2019ajout de l\u2019agent. Intrumentation inst : C\u2019est une classe native qui fournit tout un ensemble de m\u00e9thodes pour modifier le code source de l\u2019application. Mode de chargement \u00b6 Chargement statique \u00b6 L\u2019agent peut \u00eatre charg\u00e9 de mani\u00e8re dynamique , c\u2019est-\u00e0-dire au lancement de l\u2019application en rajoutant l\u2019option -javaagent indiquant le chemin vers le fichier .jar . Cette option peut \u00eatre utilis\u00e9e autant de fois que n\u00e9cessaire sur une m\u00eame ligne de commande. Il est donc possible d\u2019ex\u00e9cuter plusieurs agents sur une m\u00eame JVM. java -javaagent:path/to/agent.jar -jar application.jar Chargement dynamique \u00b6 L\u2019agent peut \u00e9galement \u00eatre charg\u00e9 de mani\u00e8re dynamique , lorsque l\u2019application est en cours d\u2019ex\u00e9cution . Ce mode de chargement se fait au niveau du code. En appelant la m\u00e9thode attach de la classe VirtualMachine avec comme argument le pid de l\u2019instance de la JVM qui ex\u00e9cute l\u2019application, on obtient une repr\u00e9sentation sous forme d\u2019objet de celle-ci. La m\u00e9thode de classe loadAgent avec le chemin de l\u2019agent comme param\u00e8tre, permet de rajouter un agent \u00e0 la JVM li\u00e9 \u00e0 l\u2019objet. Tout comme le chargement statique, il est possible de rajouter plusieurs agents \u00e0 une JVM de mani\u00e8re dynamique, en ex\u00e9cutant plusieurs fois la m\u00e9thode loadAgent . VirtualMachine virtualMachine = VirtualMachine . attach ( pid ); virtualMachine . loadAgent ( \"path/to/agent.jar\" ); virtualMachine . detach ();","title":"Agent Java"},{"location":"pages/java/java_agent/#agent-java","text":"La plupart des outils de monitoring fonctionne \u00e0 base d\u2019agent. Un agent Java est tout simplement une application Java sous forme d\u2019un fichier .jar. Utiliser un agent permet d\u2019instrumenter un programme d\u2019ex\u00e9cutant dans une JVM en interceptant le chargement des classes et en modifiant le byte code si n\u00e9cessaire.","title":"Agent java"},{"location":"pages/java/java_agent/#fichier-manifeste","text":"Une application packag\u00e9e en .jar est en g\u00e9n\u00e9ral constitu\u00e9 d\u2019un fichier MANIFEST.MF \u00e0 l\u2019int\u00e9rieur d\u2019un dossier META-INF situ\u00e9 \u00e0 la racine. Ce fichier permet de renseigner \u00e0 la JVM, les informations d\u2019ex\u00e9cution sur le fichier (classe principale, classpath, etc\u2026). Si ce fichier n\u2019est pas pr\u00e9sent, toutes les informations li\u00e9es \u00e0 l\u2019ex\u00e9cution de l\u2019application devront \u00eatre renseign\u00e9es manuellement au lancement de celle-ci. Chaque ligne correspond \u00e0 un param\u00e8tre associ\u00e9 \u00e0 sa valeur, sous la forme suivante\u202f: \u00ab\u202fparam\u00e8tre: valeur\u202f\u00bb. Le manifeste d\u2019un agent est un peu similaire \u00e0 celui d\u2019une application classique, mais avec quelque information en plus. Manifest - Version : 1.0 Premain - Class : com . orange . spirit . agent . Agent Agent - Class : com . orange . spirit . agent . Agent Main - Class : com . orange . spirit . app . Main L\u2019exemple ci-dessus nous montre le contenu du manifeste d\u2019un agent Java. Les param\u00e8tres propres \u00e0 un agent sont les suivants : Premain-Class permet d\u2019indiquer le chemin vers la classe portant la m\u00e9thode premain . Agent-Class permet d\u2019indiquer le chemin vers la classe portant la m\u00e9thode agentmain .","title":"Fichier manifeste"},{"location":"pages/java/java_agent/#methodes-premain-agentmain","text":"La m\u00e9thode premain est appel\u00e9 lorsque l\u2019agent est lanc\u00e9 de mani\u00e8re statique . Et la m\u00e9thode agentmain est appel\u00e9 lorsque l\u2019agent est lanc\u00e9 de mani\u00e8re dynamique . Les deux m\u00e9thodes constituent le point d\u2019entrer d\u2019un agent Java et ont tous deux les m\u00eames signatures. public class Agent { public static void premain ( String args , Instrumentation inst ) { // code ... } public static void agentmain ( String args , Instrumentation inst ) { // code ... } } String args : Ce sont les options pass\u00e9es en arguments dans la ligne de commande lors de l\u2019ajout de l\u2019agent. Intrumentation inst : C\u2019est une classe native qui fournit tout un ensemble de m\u00e9thodes pour modifier le code source de l\u2019application.","title":"M\u00e9thodes premain &amp; agentmain"},{"location":"pages/java/java_agent/#mode-de-chargement","text":"","title":"Mode de chargement"},{"location":"pages/java/java_agent/#chargement-statique","text":"L\u2019agent peut \u00eatre charg\u00e9 de mani\u00e8re dynamique , c\u2019est-\u00e0-dire au lancement de l\u2019application en rajoutant l\u2019option -javaagent indiquant le chemin vers le fichier .jar . Cette option peut \u00eatre utilis\u00e9e autant de fois que n\u00e9cessaire sur une m\u00eame ligne de commande. Il est donc possible d\u2019ex\u00e9cuter plusieurs agents sur une m\u00eame JVM. java -javaagent:path/to/agent.jar -jar application.jar","title":"Chargement statique"},{"location":"pages/java/java_agent/#chargement-dynamique","text":"L\u2019agent peut \u00e9galement \u00eatre charg\u00e9 de mani\u00e8re dynamique , lorsque l\u2019application est en cours d\u2019ex\u00e9cution . Ce mode de chargement se fait au niveau du code. En appelant la m\u00e9thode attach de la classe VirtualMachine avec comme argument le pid de l\u2019instance de la JVM qui ex\u00e9cute l\u2019application, on obtient une repr\u00e9sentation sous forme d\u2019objet de celle-ci. La m\u00e9thode de classe loadAgent avec le chemin de l\u2019agent comme param\u00e8tre, permet de rajouter un agent \u00e0 la JVM li\u00e9 \u00e0 l\u2019objet. Tout comme le chargement statique, il est possible de rajouter plusieurs agents \u00e0 une JVM de mani\u00e8re dynamique, en ex\u00e9cutant plusieurs fois la m\u00e9thode loadAgent . VirtualMachine virtualMachine = VirtualMachine . attach ( pid ); virtualMachine . loadAgent ( \"path/to/agent.jar\" ); virtualMachine . detach ();","title":"Chargement dynamique"},{"location":"pages/java/utils/","text":"Utiles \u00b6 Fonctions \u00b6 isStringInt \u00b6 public static boolean isStringInt ( String s ) { try { Integer . parseInt ( s ); return true ; } catch ( NumberFormatException ex ) { return false ; } } zipDirectory \u00b6 Impl\u00e9mentation Exemple public static void zipDirectory ( File folder , String parentFolder , ZipOutputStream zos ) throws IOException { for ( File file : Objects . requireNonNull ( folder . listFiles ())) { if ( file . isDirectory ()) { zipDirectory ( file , parentFolder + \"/\" + file . getName (), zos ); continue ; } zos . putNextEntry ( new ZipEntry ( parentFolder + \"/\" + file . getName ())); try ( BufferedInputStream bis = new BufferedInputStream ( new FileInputStream ( file ))) { byte [] bytesIn = new byte [ 4096 ] ; int read ; while (( read = bis . read ( bytesIn )) != - 1 ) { zos . write ( bytesIn , 0 , read ); } } } } File sourceDirectory = new File ( \"/path/to/source\" ); File targetZip = new File ( \"/path/to/target.zip\" ); try ( FileOutputStream fos = new FileOutputStream ( targetZip )) { try ( ZipOutputStream zipOut = new ZipOutputStream ( fos )) { zipDirectory ( sourceDirectory , sourceDirectory . getName (), zipOut ); } }","title":"Utiles"},{"location":"pages/java/utils/#utiles","text":"","title":"Utiles"},{"location":"pages/java/utils/#fonctions","text":"","title":"Fonctions"},{"location":"pages/java/utils/#isstringint","text":"public static boolean isStringInt ( String s ) { try { Integer . parseInt ( s ); return true ; } catch ( NumberFormatException ex ) { return false ; } }","title":"isStringInt"},{"location":"pages/java/utils/#zipdirectory","text":"Impl\u00e9mentation Exemple public static void zipDirectory ( File folder , String parentFolder , ZipOutputStream zos ) throws IOException { for ( File file : Objects . requireNonNull ( folder . listFiles ())) { if ( file . isDirectory ()) { zipDirectory ( file , parentFolder + \"/\" + file . getName (), zos ); continue ; } zos . putNextEntry ( new ZipEntry ( parentFolder + \"/\" + file . getName ())); try ( BufferedInputStream bis = new BufferedInputStream ( new FileInputStream ( file ))) { byte [] bytesIn = new byte [ 4096 ] ; int read ; while (( read = bis . read ( bytesIn )) != - 1 ) { zos . write ( bytesIn , 0 , read ); } } } } File sourceDirectory = new File ( \"/path/to/source\" ); File targetZip = new File ( \"/path/to/target.zip\" ); try ( FileOutputStream fos = new FileOutputStream ( targetZip )) { try ( ZipOutputStream zipOut = new ZipOutputStream ( fos )) { zipDirectory ( sourceDirectory , sourceDirectory . getName (), zipOut ); } }","title":"zipDirectory"},{"location":"pages/linux/ssh/","text":"SSH \u00b6 Installer SSH Server \u00b6 apt-get update \\ && apt-get install -y openssh-server \\ && dpkg-reconfigure openssh-server \\ && service ssh start Cr\u00e9er un tunnel SSH \u00b6 Syntaxe ssh -f <username>@<remote_host> -L <local_port>:localhost:<destination_port> -N Exemple ssh -f rudmls@172.17.75.218 -L 8070:localhost:8070 -N","title":"SSH"},{"location":"pages/linux/ssh/#ssh","text":"","title":"SSH"},{"location":"pages/linux/ssh/#installer-ssh-server","text":"apt-get update \\ && apt-get install -y openssh-server \\ && dpkg-reconfigure openssh-server \\ && service ssh start","title":"Installer SSH Server"},{"location":"pages/linux/ssh/#creer-un-tunnel-ssh","text":"Syntaxe ssh -f <username>@<remote_host> -L <local_port>:localhost:<destination_port> -N Exemple ssh -f rudmls@172.17.75.218 -L 8070:localhost:8070 -N","title":"Cr\u00e9er un tunnel SSH"},{"location":"pages/linux/utils/","text":"Utiles \u00b6 Constantes \u00b6 Liste des utilisateurs \u00b6 awk -F: '{print $1}' /etc/passwd Liste des variables d'environnements \u00b6 env | awk -F= '{print $1}' Fonctions \u00b6 is_mounted function is_mounted () { [ ! -z $( mount | awk '{print $3}' | grep ^ $1 $ ) ] } is_user_root function is_user_root () { [ \" $( id -u ) \" -eq 0 ] } is_package_exist function is_package_exist () { [ ! -z $( dpkg -l | awk '{print $2}' | grep ^ $1 $ ) ] } get_env_paths function get_env_paths () { echo $( env | grep $1 | awk -F = '{print $2}' | tr ':' '\\n' ) } get_packages_like function get_packages_like () { echo $( dpkg -l | awk '{print $2}' | grep $1 ) } delete_group function delete_group () { if [ $( getent group $1 ) ] ; then groupdel $1 fi }","title":"Utiles"},{"location":"pages/linux/utils/#utiles","text":"","title":"Utiles"},{"location":"pages/linux/utils/#constantes","text":"","title":"Constantes"},{"location":"pages/linux/utils/#liste-des-utilisateurs","text":"awk -F: '{print $1}' /etc/passwd","title":"Liste des utilisateurs"},{"location":"pages/linux/utils/#liste-des-variables-denvironnements","text":"env | awk -F= '{print $1}'","title":"Liste des variables d'environnements"},{"location":"pages/linux/utils/#fonctions","text":"is_mounted function is_mounted () { [ ! -z $( mount | awk '{print $3}' | grep ^ $1 $ ) ] } is_user_root function is_user_root () { [ \" $( id -u ) \" -eq 0 ] } is_package_exist function is_package_exist () { [ ! -z $( dpkg -l | awk '{print $2}' | grep ^ $1 $ ) ] } get_env_paths function get_env_paths () { echo $( env | grep $1 | awk -F = '{print $2}' | tr ':' '\\n' ) } get_packages_like function get_packages_like () { echo $( dpkg -l | awk '{print $2}' | grep $1 ) } delete_group function delete_group () { if [ $( getent group $1 ) ] ; then groupdel $1 fi }","title":"Fonctions"}]}